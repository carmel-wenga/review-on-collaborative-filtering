{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MzSv49iMF1_"
   },
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGoHyoipMF2B"
   },
   "source": [
    "<b>User-based</b> and <b>Item-based</b> collaborative Filtering recommender systems suffer from <i>data sparsity</i> and <i>scalability</i> for online recommendations. <b>Matrix Factorization</b> helps to address these drawbacks of memory-based collaborative filtering by reducing the dimension of the rating matrix $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSvTMCyVMF2C"
   },
   "source": [
    "The movielen lasted small dataset has 100k ratings of $m=610$ users on $n=9724$ items. The rating matrix in then a $m\\times n$ matrix (i.e $R\\in \\mathbb{R}^{m\\times n}$). The fact that users usually interact with less than $1\\%$ of items leads the rating matrix $R$ to be highly sparse. For example, the degree of sparsity of the movielen lasted small dataset is \n",
    "\n",
    "\\begin{equation}\n",
    "sparsity = 100 - \\frac{\\text{total # ratings}}{m \\times n} = 100 - \\frac{100000}{610\\times 9724} = 98,3\\%\n",
    "\\end{equation}\n",
    "\n",
    "This means that in this dataset, a user has interacted with less than $2\\%$ of items. To reduce the dimension of the rating matrix $R$, Matrix Factorization (MF) mappes both users and items to a joint latent factor space of dimensionality $k$ such that user-item interactions are modeled as inner products in that space <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>. MF then decomposes $R$ in two matrices as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "R = Q^\\top P\n",
    "\\end{equation}\n",
    "\n",
    "Where $P \\in \\mathbb{R}^{m\\times k}$ represents latent factors of users and $Q \\in \\mathbb{R}^{n\\times k}$ is the latent factors of items. Each line of $P$, say $p_u \\in \\mathbb{R}^k$ denotes the taste of user $u$ and each $q_i \\in \\mathbb{R}^k$ the features of item $i$. The dot product between $p_u$ and $q_i$ will be the rating prediction of user $u$ on item $i$ :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{r}_{u,i} = q_{i}^{\\top} p_u.\n",
    "\\end{equation}\n",
    "\n",
    "Figure 1 presents an example of decomposition of $R$ into two matrices $P$ and $Q$.\n",
    "\n",
    "<img src=\"tools/img/MF.png\" width=\"650px\">\n",
    "<br>\n",
    "<center><b>Figure 1</b>: Decomposition of $R$ into $P$ and $Q$</center>\n",
    "\n",
    "\n",
    "To learn the latent factors $p_u$ and $q_i$, the system minimizes the regularized squared error on the set of known ratings. The cost function $J$ is defined as follows : \n",
    "\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{2}\\sum_{(u,i)\\in \\kappa} (r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\kappa$ is the set of $(u,i)$ pairs for which $r_{u,i}$ is known (the training set), and $\\lambda$ is the regularizer parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azVL_VTEMF2C"
   },
   "source": [
    "## Learning Algorithms\n",
    "\n",
    "As described in <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>, to minimize the cost function $J$, the matrix factorization algorithm predicts $\\hat{r}_{u,i}$ for each given training case (existing $r_{u,i}$), and computes the associated error defined by the Mean Absolute Error (MAE) as :\n",
    "\n",
    "\\begin{equation}\n",
    "e_{u,i} = |r_{ui} - q_{i}^{\\top} p_u|.\n",
    "\\end{equation}\n",
    "\n",
    "<b>Note</b> : The overall error $E$ is defined as :\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{M}\\sum_{(u,i)\\in\\kappa} e_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "Where $M$ is the number of example. The update rules for parameters $p_u$ and $q_i$ are defined as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i - \\alpha\\frac{\\partial}{\\partial q_i}J_{u,i}, \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u - \\alpha\\frac{\\partial}{\\partial p_u}J_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial}{\\partial p_u}J_{u,i}$ is the partial derivative of the cost function $J$ according to $p_u$. It computes the extent to which $p_u$ contributes to the total error.\n",
    "\n",
    "### How to compute $\\frac{\\partial}{\\partial q_i}J_{u,i}$ ?\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial q_i}J_{u,i} & = & \\frac{1}{2}\\frac{\\partial}{\\partial q_i} \\begin{bmatrix}(r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\end{bmatrix} \\\\\n",
    "& = & -(r_{u,i}-q_{i}^{\\top} p_u)\\cdot p_u + \\lambda \\cdot q_i  \\\\\n",
    "& = & -e_{u,i}\\cdot p_u+\\lambda \\cdot q_i\n",
    "\\end{align}\n",
    "\n",
    "The update rules are then given by : \n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i), \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04D2zA48MF2D"
   },
   "source": [
    "## Matrix Factorization : algorithm\n",
    "<ol>\n",
    "    <li>Initialize $P$ and $Q$ with random values\n",
    "    <li>For each training example $(u,i)\\in\\kappa$ with the corresponding rating $r_{u,i}$ :\n",
    "        <ul>\n",
    "            <li>compute $\\hat{r}_{u,i}$ as $\\hat{r}_{u,i} = q_{i}^{\\top} p_u$\n",
    "            <li>compute the error : $e_{u,i} = |r_{ui} - \\hat{r}_{u,i}|$\n",
    "            <li>update $p_u$ and $q_i$:\n",
    "                <ul>\n",
    "                    <li>$p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)$\n",
    "                    <li>$q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i)$\n",
    "                </ul>\n",
    "        </ul>\n",
    "    <li> Repeat step 2 until the optimal parameters are reached.\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4k3EF8VMF2D"
   },
   "source": [
    "## Matrix Factorization : implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnzHRemEMF2E"
   },
   "source": [
    "Let's start by importing useful requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vM3wg1cPMF2F"
   },
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_ohlWURMF2F"
   },
   "outputs": [],
   "source": [
    "from tools.utils import download_data, load_movies, load_ratings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYc9c_YoMF2I"
   },
   "source": [
    "Download movielen data if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gCrrdOF7MF2J",
    "outputId": "5fffe8ed-2559-48d5-d28b-4baa4822432b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists ...\n"
     ]
    }
   ],
   "source": [
    "data = os.path.join('tools', 'ml-latest-small')\n",
    "\n",
    "if os.path.exists(data):\n",
    "    print('Data already exists ...')\n",
    "    ratings_csv, movies_csv = os.path.join(data, 'ratings.csv'), os.path.join(data, 'movies.csv')\n",
    "else:\n",
    "    ratings_csv, movies_csv = download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7XA13VGMF2N"
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4s3z1vBMF2N"
   },
   "outputs": [],
   "source": [
    "ratings, movies = load_ratings(ratings_csv), load_movies(movies_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXSmGfAeMF2Q"
   },
   "source": [
    "Let's generate our rating matrix from from the ```ratings``` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPf9iC3AMF2R"
   },
   "outputs": [],
   "source": [
    "def ratings_matrix(ratings):\n",
    "    \"\"\"\n",
    "    :param ratings : DataFrame of user ratings\n",
    "    :return R : numpy array of shape (m,n). where m is the number of users and n the number of items\n",
    "    \"\"\"\n",
    "    \n",
    "    R = csr_matrix(pd.crosstab(ratings.userid, ratings.itemid, ratings.rating, aggfunc=sum).fillna(0).values)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IZ-mfSyFMF2T",
    "outputId": "c42baf20-8b88-4d65-8fcd-75d37f9a5422"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = ratings_matrix(ratings)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iGmYxndMF2X"
   },
   "source": [
    "As expected, the shape of $R$ is (610, 9724). ```R[0,:]``` (resp. ```R[909,:]```) represents ratings given by the first user with id=1 (resp. the last user with id=910) to all items. Similarily, ```R[:,0]``` (resp. ```R[:,9723]```) represents ratings given by all users to the first item with index 0 (resp. to the last items with index 9723).\n",
    "\n",
    "We have 9724 items, but their ids do not ranges from 1 to 9724 but from 1 to 193609. So our item ids are 9724 values taken in the range 1 to 193609. In order to be able to access ratings of users or items through the rating matrix $R$, we need to encode their ids in such a way that they range from 0 to 909 for users and from 0 to 9723 for items. To do that, we will user the ```LabelEncoder``` of the ```sklearn``` library.\n",
    "\n",
    "Moreover, we need to split our dataset into train and test sets for training and validation steps respectively.\n",
    "1. ```examples``` is the set of $(u,i)$ pairs for which $r_{u,i}$ is known.\n",
    "2. ```labels``` set of existing ratings $r_{u,i}$ for all $(u,i)$ pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YFNAD9BMF2X"
   },
   "outputs": [],
   "source": [
    "examples = ratings[['userid', 'itemid']].values\n",
    "labels = ratings.rating.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fah7D6czMF2b"
   },
   "source": [
    "Now let's split our training ```examples``` as well as their ```labels``` into train and validation sets using the ```train_test_split``` function of the ```sklearn``` library. The details about splitting are the following:\n",
    "\n",
    "- ```test_size = 0.1``` specifies that the size of the test set will account for $10\\%$ of the data set and that the training set will take $100\\%-10\\%=90\\%$. \n",
    "- With the ```shuffle``` parameter assigned to ```True```, the dataset will be randomly shuffled before the train/test split operation.\n",
    "- The ```random_state``` controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6W_Ov-IMF2d"
   },
   "outputs": [],
   "source": [
    "train_examples, test_examples, train_labels, test_labels = train_test_split(\n",
    "    examples, \n",
    "    labels, \n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHxC2iz8MF2g"
   },
   "source": [
    "Now let's encode our training and validation examples to be able to access their corresponding latent factors in $P$ and $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3NDlx-xMF2g"
   },
   "outputs": [],
   "source": [
    "users = sorted(ratings['userid'].unique())\n",
    "items = sorted(ratings['itemid'].unique())\n",
    "\n",
    "# create our id encoders\n",
    "uencoder = LabelEncoder()\n",
    "iencoder = LabelEncoder()\n",
    "\n",
    "# fit our label encoder\n",
    "uencoder.fit(users)\n",
    "iencoder.fit(items)\n",
    "\n",
    "# transform train and test examples to their corresponding one-hot representations\n",
    "train_users = train_examples[:,0]\n",
    "test_users = test_examples[:,0]\n",
    "\n",
    "train_items = train_examples[:,1]\n",
    "test_items = test_examples[:,1]\n",
    "\n",
    "train_users = uencoder.transform(train_users)\n",
    "test_users = uencoder.transform(test_users)\n",
    "\n",
    "train_items = iencoder.transform(train_items)\n",
    "test_items = iencoder.transform(test_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blg8idyJMF2j"
   },
   "source": [
    "Final training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shJcoQQTMF2k"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = np.array(list(zip(train_users, train_items))), np.array(list(zip(test_users, test_items)))\n",
    "y_train, y_test = train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "sT8MUMCKMF2o",
    "outputId": "73e73a2d-6a64-4d7a-a510-4ea4b15b1f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users :  610\n",
      "number of items :  9724\n"
     ]
    }
   ],
   "source": [
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "print('number of users : ', m)\n",
    "print('number of items : ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3dz9j0IMF2s"
   },
   "source": [
    "### Matrix Factorization Model\n",
    "\n",
    "Now let's define our matrix factorization model. Here are the methods associated to our matrix factorization model\n",
    "\n",
    "1. ```fit(x_train, y_train, epochs, validation_data)```: train the matrix factorization model (train the latent factors $P$ and $Q$) on the training set for 1000 epochs by default and use the test set as validation data.\n",
    "2. ```evaluate(x_test, y_test)```: evaluate the model on the test set\n",
    "3. ```predict(userid, itemid)```: make rating prediction for a user on a given item using the learnt latent factors $P$ and $Q$.\n",
    "4. ```recommend(userid, N=30)```: make top $N$ recommendation for a given user. Return the top $N$ items with the $N$ largest predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29phzvm_MF2t"
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    \n",
    "    def __init__(self, R, m, n, k=50, alpha=0.1, lamb=1):\n",
    "        \"\"\"\n",
    "        Initialization of the model        \n",
    "        :param\n",
    "            - R : rating matrix of shape (m,n)\n",
    "            - m : number of users\n",
    "            - n : number of items\n",
    "            - k : length of latent factor, both for users and items. 50 by default\n",
    "            - alpha : learning rate. 0.001 by default\n",
    "            - lamb : regularizer parameter. 0.02 by default\n",
    "        \"\"\"\n",
    "        # rating matrix\n",
    "        self.R = R\n",
    "        \n",
    "        np.random.seed(32)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.k = k\n",
    "        self.P = np.random.normal(size=(m,k))\n",
    "        self.Q = np.random.normal(size=(n,k))\n",
    "        \n",
    "        # hyperparameter initialization\n",
    "        self.alpha = alpha\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\":[],\n",
    "            \"loss\":[],\n",
    "            \"val_loss\":[]\n",
    "        }\n",
    "    \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train latent factors P and Q according to the training set\n",
    "        \n",
    "        :param\n",
    "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
    "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
    "            - validation_data : tuple (x_test, y_test)\n",
    "            - epochs : number of time to loop over the entire training set. \n",
    "            1000 epochs by default\n",
    "            \n",
    "        Note that u and i are encoded values of userid and itemid\n",
    "        \"\"\"\n",
    "        \n",
    "        print('[INFO] Training Matrix Factorization with the following hyperparameters :\\n')\n",
    "        print('   - Number of latent factor k = {}'.format(self.k))\n",
    "        print('   - learning rate alpha = {}'.format(self.alpha))\n",
    "        print('   - regularization parameter lamb = {} \\n'.format(self.lamb))\n",
    "        print('[INFO] Strating training job ... \\n')\n",
    "\n",
    "        # number of training examples\n",
    "        M = x_train.shape[0]\n",
    "        \n",
    "        # loop over the number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # for each pair (u,i) and the corresponding rating r\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                \n",
    "                # get encoded values of userid and itemid from pair\n",
    "                u,i = pair\n",
    "                \n",
    "                # compute the predicted rating r_hat\n",
    "                r_hat = np.dot(self.P[u], self.Q[i])\n",
    "                \n",
    "                # compute the prediction error\n",
    "                e = abs(r - r_hat)\n",
    "                \n",
    "                # update rules\n",
    "                # P[u,:] = P[u,:] + alpha * (e * Q[i,:] - lambda * P[u,:])\n",
    "                self.P[u] = self.P[u] + self.alpha * (e * self.Q[i] - self.lamb * self.P[u])\n",
    "                self.Q[i] = self.Q[i] + self.alpha * (e * self.P[u] - self.lamb * self.Q[i])\n",
    "                \n",
    "            # global training error after this epochs\n",
    "            error = 0\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                error += abs(r - np.dot(self.P[pair[0]], self.Q[pair[1]]))\n",
    "            error /= M\n",
    "\n",
    "            # error on validation set\n",
    "            x_test, y_test = validation_data\n",
    "            val_error = 0\n",
    "            for pair, r in zip(x_test, y_test):\n",
    "                val_error += abs(r - np.dot(self.P[pair[0]], self.Q[pair[1]]))\n",
    "            val_error /= x_test.shape[0]\n",
    "            \n",
    "            # update history\n",
    "            self.history['epochs'].append(epoch)\n",
    "            self.history['loss'].append(error)\n",
    "            self.history['val_loss'].append(val_error)\n",
    "            \n",
    "            # print training progress after each 10 epochs\n",
    "            if epoch % 5 == 0 or epoch == epochs-1:\n",
    "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
    "              \n",
    "            # leaning rate scheduler : redure the learning rate as we go deeper in the number of epochs\n",
    "            if epoch >= 30 and epoch % 30 == 0:\n",
    "                factor = epoch // 30\n",
    "                self.alpha = self.alpha * (1 / (factor * 20))\n",
    "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        compute the global error on the test set\n",
    "        \n",
    "        :param\n",
    "            - x_test : test pairs (u,i) for which rating r_ui is known\n",
    "            - y_test : set of ratings r_ui for all test pairs (u,i)\n",
    "        \"\"\"\n",
    "        # number of test examples\n",
    "        M = x_test.shape[0]\n",
    "        \n",
    "        # initialize the error\n",
    "        error = 0\n",
    "        for pair, r in zip(x_test, y_test):\n",
    "            u,i = pair\n",
    "            e = abs(r - np.dot(self.P[u,:], self.Q[i,:]))\n",
    "            error += e\n",
    "        \n",
    "        error /= M\n",
    "        print(\"val_error : \", str(round(error,3)))\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "        \"\"\"\n",
    "        Make rating prediction for a user on an item\n",
    "\n",
    "        :param\n",
    "        - userid\n",
    "        - itemid\n",
    "\n",
    "        :return\n",
    "        - r : predicted rating\n",
    "        \"\"\"\n",
    "        # encode user and item ids to be able to access their latent factors in\n",
    "        # matrices P and Q\n",
    "        u = uencoder.transform([userid])[0]\n",
    "        i = iencoder.transform([itemid])[0]\n",
    "\n",
    "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "        \"\"\"\n",
    "        make to N recommendations for a given user\n",
    "\n",
    "        :return \n",
    "        - (top_items,preds) : top N items with the highest predictions \n",
    "        with their corresponding predictions\n",
    "        \"\"\"\n",
    "        # encode the userid\n",
    "        u = uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for users userid on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82vmS-bpMF2w"
   },
   "source": [
    "Now that the model has been defined, let's fit it with the training set. The efficiency of the model will highly depends on hyperparameters tuning : find optimal values of $k$ (number of latent factors both for users and items), $\\alpha$ (```alpha```, the learning rate) and $\\lambda$ (```lamb```, the regularizer parameter).\n",
    "\n",
    "We have to tune the tuple $(k, \\alpha, \\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "evNl8i1KMF2w",
    "outputId": "664f7c83-9bc7-44ca-a694-89864a16685c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training Matrix Factorization with the following hyperparameters :\n",
      "\n",
      "   - Number of latent factor k = 20\n",
      "   - learning rate alpha = 0.005\n",
      "   - regularization parameter lamb = 1.23 \n",
      "\n",
      "[INFO] Strating training job ... \n",
      "\n",
      "epoch 0/41 - loss : 3.364 - val_loss : 3.47\n",
      "epoch 5/41 - loss : 1.919 - val_loss : 2.0\n",
      "epoch 10/41 - loss : 1.574 - val_loss : 1.646\n",
      "epoch 15/41 - loss : 1.452 - val_loss : 1.523\n",
      "epoch 20/41 - loss : 1.389 - val_loss : 1.459\n",
      "epoch 25/41 - loss : 1.349 - val_loss : 1.42\n",
      "epoch 30/41 - loss : 1.323 - val_loss : 1.386\n",
      "\n",
      "Learning Rate : 0.00025\n",
      "\n",
      "epoch 35/41 - loss : 1.317 - val_loss : 1.375\n",
      "epoch 40/41 - loss : 1.315 - val_loss : 1.361\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(R, m, n, k=20, alpha=0.005, lamb=1.23)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=41, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "varg8U-Dy_ar"
   },
   "source": [
    "Let's visualize the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "eJ97JNkXzl3A",
    "outputId": "ca4e7176-9af0-4455-8893-b316aa87602f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(MF.history['epochs'], MF.history['loss'], 'r', label=\"loss\")\n",
    "plt.plot(MF.history['epochs'], MF.history['val_loss'], 'b', label=\"val_loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MAE (Mean Absolute Error)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Bc6F0HWMWbLd",
    "outputId": "8e271e4a-ca53-4d05-eac3-90d18baf4ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_error :  1.361\n"
     ]
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hzclXP0RL8V"
   },
   "source": [
    "Now that the latent factors $P$ and $Q$, we can use them to make predictions and recommendations. Let's call the ```predict``` function of the ```Matrix Factorization``` class to make prediction for a given.\n",
    "\n",
    "rating prediction for user 1 on item 6 for which the truth rating $r=4.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WjQN_N1WRKSm",
    "outputId": "0003d90b-3c19-4af8-c77f-5206a60d5be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.269817544036902"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.predict(userid=1, itemid=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYncVXWvebEZ"
   },
   "source": [
    "Now we can make recommendations for a  given user with function ```recommend``` of the ```Matrix Factorization``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "colab_type": "code",
    "id": "M5FpA7T3uA2v",
    "outputId": "35730e8b-e6d8-4f56-f6f4-161e09128412"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>predictions</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97194</td>\n",
       "      <td>5.436833</td>\n",
       "      <td>Thing: Terror Takes Shape, The (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71033</td>\n",
       "      <td>4.993889</td>\n",
       "      <td>Secret in Their Eyes, The (El secreto de sus o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7065</td>\n",
       "      <td>4.973198</td>\n",
       "      <td>Birth of a Nation, The (1915)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8464</td>\n",
       "      <td>4.883735</td>\n",
       "      <td>Super Size Me (2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92259</td>\n",
       "      <td>4.812528</td>\n",
       "      <td>Intouchables (2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1203</td>\n",
       "      <td>4.643011</td>\n",
       "      <td>12 Angry Men (1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5444</td>\n",
       "      <td>4.627319</td>\n",
       "      <td>Lilo &amp; Stitch (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6016</td>\n",
       "      <td>4.597095</td>\n",
       "      <td>City of God (Cidade de Deus) (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>335</td>\n",
       "      <td>4.519516</td>\n",
       "      <td>Underneath (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54286</td>\n",
       "      <td>4.464704</td>\n",
       "      <td>Bourne Ultimatum, The (2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5618</td>\n",
       "      <td>4.418804</td>\n",
       "      <td>Spirited Away (Sen to Chihiro no kamikakushi) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>178</td>\n",
       "      <td>4.390369</td>\n",
       "      <td>Love &amp; Human Remains (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3450</td>\n",
       "      <td>4.325613</td>\n",
       "      <td>Grumpy Old Men (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>106642</td>\n",
       "      <td>4.294706</td>\n",
       "      <td>Day of the Doctor, The (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>508</td>\n",
       "      <td>4.274174</td>\n",
       "      <td>Philadelphia (1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>134326</td>\n",
       "      <td>4.225737</td>\n",
       "      <td>The Taming of the Scoundrel (1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47610</td>\n",
       "      <td>4.221846</td>\n",
       "      <td>Illusionist, The (2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>136958</td>\n",
       "      <td>4.219179</td>\n",
       "      <td>Mortuary (1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2248</td>\n",
       "      <td>4.180741</td>\n",
       "      <td>Say Anything... (1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>56152</td>\n",
       "      <td>4.174128</td>\n",
       "      <td>Enchanted (2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>96821</td>\n",
       "      <td>4.171290</td>\n",
       "      <td>Perks of Being a Wallflower, The (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3586</td>\n",
       "      <td>4.163248</td>\n",
       "      <td>The Idolmaker (1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>95583</td>\n",
       "      <td>4.101549</td>\n",
       "      <td>Savages (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1035</td>\n",
       "      <td>4.082255</td>\n",
       "      <td>Sound of Music, The (1965)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>138835</td>\n",
       "      <td>4.066348</td>\n",
       "      <td>Return to Treasure Island (1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2762</td>\n",
       "      <td>4.053777</td>\n",
       "      <td>Sixth Sense, The (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>176579</td>\n",
       "      <td>4.051202</td>\n",
       "      <td>Cage Dive (2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45517</td>\n",
       "      <td>4.048681</td>\n",
       "      <td>Cars (2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>745</td>\n",
       "      <td>4.028067</td>\n",
       "      <td>Wallace &amp; Gromit: A Close Shave (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>357</td>\n",
       "      <td>4.021041</td>\n",
       "      <td>Four Weddings and a Funeral (1994)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    itemid  predictions                                              title\n",
       "0    97194     5.436833              Thing: Terror Takes Shape, The (1998)\n",
       "1    71033     4.993889  Secret in Their Eyes, The (El secreto de sus o...\n",
       "2     7065     4.973198                      Birth of a Nation, The (1915)\n",
       "3     8464     4.883735                               Super Size Me (2004)\n",
       "4    92259     4.812528                                Intouchables (2011)\n",
       "5     1203     4.643011                                12 Angry Men (1957)\n",
       "6     5444     4.627319                               Lilo & Stitch (2002)\n",
       "7     6016     4.597095                City of God (Cidade de Deus) (2002)\n",
       "8      335     4.519516                                  Underneath (1995)\n",
       "9    54286     4.464704                       Bourne Ultimatum, The (2007)\n",
       "10    5618     4.418804  Spirited Away (Sen to Chihiro no kamikakushi) ...\n",
       "11     178     4.390369                        Love & Human Remains (1993)\n",
       "12    3450     4.325613                              Grumpy Old Men (1993)\n",
       "13  106642     4.294706                      Day of the Doctor, The (2013)\n",
       "14     508     4.274174                                Philadelphia (1993)\n",
       "15  134326     4.225737                 The Taming of the Scoundrel (1980)\n",
       "16   47610     4.221846                            Illusionist, The (2006)\n",
       "17  136958     4.219179                                    Mortuary (1983)\n",
       "18    2248     4.180741                             Say Anything... (1989)\n",
       "19   56152     4.174128                                   Enchanted (2007)\n",
       "20   96821     4.171290            Perks of Being a Wallflower, The (2012)\n",
       "21    3586     4.163248                               The Idolmaker (1980)\n",
       "22   95583     4.101549                                     Savages (2012)\n",
       "23    1035     4.082255                         Sound of Music, The (1965)\n",
       "24  138835     4.066348                   Return to Treasure Island (1988)\n",
       "25    2762     4.053777                            Sixth Sense, The (1999)\n",
       "26  176579     4.051202                                   Cage Dive (2017)\n",
       "27   45517     4.048681                                        Cars (2006)\n",
       "28     745     4.028067             Wallace & Gromit: A Close Shave (1995)\n",
       "29     357     4.021041                 Four Weddings and a Funeral (1994)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of top N items with their corresponding predicted ratings\n",
    "recommended_items, predictions = MF.recommend(userid=42)\n",
    "\n",
    "# find corresponding movie titles\n",
    "top_N = list(zip(recommended_items,predictions))\n",
    "top_N = pd.DataFrame(top_N, columns=['itemid','predictions'])\n",
    "List = pd.merge(top_N, movies, on='itemid', how='inner')\n",
    "\n",
    "# show the list\n",
    "List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPeVMap_wu2T"
   },
   "source": [
    "**Note**: The recommendation list may content items already purchased by the user. This is just an illustration of how to implement matrix factorization recommender system. You can optimize the recommended list and return the top rated items that the user has not already purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-jjxDhex5Gs"
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. Yehuda Koren et al. (2009). <a href='https://ieeexplore.ieee.org/document/5197422'>Matrix Factorization Techniques for Recommender Systems</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jV3Jj17ox_UR"
   },
   "source": [
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/carmel-wenga-871876178/\">Carmel WENGA</a>, Applied Machine Learning Research Engineer | <a href=\"https://shoppinglist.cm/fr/\">ShoppingList</a>, Nzhinusoft"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. matrix factorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
