{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the following link with a new tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nzhinusoftcm/review-on-collaborative-filtering/blob/master/7.Explainable_Matrix_Factorization.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MzSv49iMF1_"
   },
   "source": [
    "# Explainable Matrix Factorization (EMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to quantify explainability ?\n",
    "\n",
    "- Use the rating distribution within the active userâ€™s neighborhood. \n",
    "- If many neighbors have rated the recommended item, then this can provide a basis upon which to explain the recommendations, using neighborhood style explanation mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [(Abdollahi and Nasraoui, 2016)](https://www.researchgate.net/publication/301616080_Explainable_Matrix_Factorization_for_Collaborative_Filtering), an item $i$ is consider to be explainable for user $u$ if a considerable number of its neighbors rated item $i$. The explainability score $E_{ui}$ is the percentage of user $u$'s neighbors who have rated item $i$.\n",
    "\n",
    "\\begin{equation}\n",
    "E_{ui} = \\frac{|N_k^{(i)}(u)|}{|N_k(u)|},\n",
    "\\end{equation}\n",
    "\n",
    "where $N_k(u)$ is the set of $k$ nearest neighbors of user $u$ and $N_k^{(i)}(u)$ is the set of user $u$'s neighbors who have rated item $i$. However, only explainable scores above an optimal threshold $\\theta$ are accepted.\n",
    "\n",
    "\\begin{equation}\n",
    "W_{ui} = \\begin{cases} E_{ui} \\text{  } if \\text{  } E_{ui} > \\theta \\\\ 0 \\text{ } otherwise \\end{cases},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including explainability weight in the training algorithm, the new objective function, to be minimized over the set of known ratings, has been formulated by [(Abdollahi and Nasraoui, 2016)](https://www.researchgate.net/publication/301616080_Explainable_Matrix_Factorization_for_Collaborative_Filtering) as:\n",
    "\n",
    "\\begin{equation}\n",
    " J = \\sum_{(u,i)\\in \\kappa} (R_{ui} - \\hat{R}_{ui})^2 +\\frac{\\beta}{2}(||P_u||^2 + ||Q_i||^2) + \\frac{\\lambda}{2}(P_u-Q_i)^2W_{ui},\n",
    "\\end{equation}\n",
    "\n",
    "here, $\\frac{\\beta}{2}(||P_u||^2 + ||Q_i||^2)$ is the $L_2$ regularization term weighted by the coefficient $\\beta$, and $\\lambda$ is an explainability regularization coefficient that controls the smoothness of the new representation and tradeoff between explainability and accuracy. The idea here is that if item $i$ is explainable for user $u$, then their representations in the latent space, $Q_i$ and $P_u$, should be close to each other. Stochastic Gradient descent can be used to optimize the objectve function.\n",
    "\n",
    "\\begin{equation}\n",
    "P_u \\leftarrow P_u + \\alpha\\left(2(R_{u,i}-P_uQ_i^{\\top})Q_i - \\beta P_u - \\lambda(P_u-Q_i)W_{ui}\\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Q_i \\leftarrow Q_i + \\alpha\\left(2(R_{u,i}-P_uQ_i^{\\top})P_u - \\beta Q_i + \\lambda(P_u-Q_i)W_{ui}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4k3EF8VMF2D"
   },
   "source": [
    "## Explainable Matrix Factorization : Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
    "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
    "    !unzip recsys.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements\n",
    "\n",
    "```\n",
    "matplotlib==3.2.2\n",
    "numpy==1.18.1\n",
    "pandas==1.0.5\n",
    "python==3.6.10\n",
    "scikit-learn==0.23.1\n",
    "scipy==1.5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_ohlWURMF2F"
   },
   "outputs": [],
   "source": [
    "from recsys.memories.UserToUser import UserToUser\n",
    "\n",
    "from recsys.preprocessing import mean_ratings\n",
    "from recsys.preprocessing import normalized_ratings\n",
    "from recsys.preprocessing import  encode_data\n",
    "from recsys.preprocessing import split_data\n",
    "from recsys.preprocessing import rating_matrix\n",
    "from recsys.preprocessing import get_examples\n",
    "\n",
    "from recsys.datasets import ml100k, ml1m, mlLastedSmall\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Explainable Scores\n",
    "\n",
    "Explainable score are computed using neighborhood based similarities. Here, we are using the user based algorithme to compute similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainable_score(user2user, users, items, theta=0):\n",
    "        \n",
    "    # initialize explainable score to zeros\n",
    "    W = np.zeros((len(users), len(items)))\n",
    "\n",
    "    for u in range(len(users)):            \n",
    "        user_neighbors = user2user.neighbors[u][1:]\n",
    "        candidate_items = user2user.find_user_candidate_items(u,user_neighbors)\n",
    "        candidate_items = iencoder.transform(candidate_items)\n",
    "\n",
    "        for i in candidate_items:                \n",
    "            user_who_rated_i, similar_user_who_rated_i = \\\n",
    "                user2user.similar_users_who_rated_this_item(i, user_neighbors)\n",
    "\n",
    "            if len(user_who_rated_i) == 0:\n",
    "                w = 0.0\n",
    "            else:\n",
    "                w = len(similar_user_who_rated_i) / len(user_who_rated_i)\n",
    "\n",
    "            W[u,i] =  w  if w > theta else 0.0 \n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainable Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableMatrixFactorization:\n",
    "    \n",
    "    def __init__(self, m, n, W, alpha=0.001, beta=0.01, lamb=0.1, k=10):\n",
    "        \"\"\"\n",
    "            - R : Rating matrix of shape (m,n) \n",
    "            - W : Explainability Weights of shape (m,n)\n",
    "            - k : number of latent factors\n",
    "            - beta : L2 regularization parameter\n",
    "            - lamb : explainability regularization coefficient\n",
    "            - theta : threshold above which an item is explainable for a user\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        \n",
    "        np.random.seed(64)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.k = k\n",
    "        self.P = np.random.normal(size=(self.m,k))\n",
    "        self.Q = np.random.normal(size=(self.n,k))\n",
    "        \n",
    "        # hyperparameter initialization\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\":[],\n",
    "            \"loss\":[],\n",
    "            \"val_loss\":[],\n",
    "        }\n",
    "        \n",
    "    def print_training_parameters(self):\n",
    "        print('Training EMF')\n",
    "        print(f'k={self.k} \\t alpha={self.alpha} \\t beta={self.beta} \\t lambda={self.lamb}')\n",
    "        \n",
    "    def update_rule(self, u, i, error):\n",
    "        self.P[u] = self.P[u] + \\\n",
    "            self.alpha * ( 2 * error * self.Q[i] - self.beta * self.P[u] - self.lamb * ( self.P[u] - self.Q[i] ) * self.W[u,i])\n",
    "        \n",
    "        self.Q[i] = self.Q[i] + \\\n",
    "            self.alpha * (2 * error * self.P[u] - self.beta * self.Q[i] + self.lamb * ( self.P[u] - self.Q[i] ) * self.W[u,i])\n",
    "        \n",
    "    def mae(self,  x_train, y_train):\n",
    "        \"\"\"\n",
    "        returns the Mean Absolute Error\n",
    "        \"\"\"\n",
    "        # number of training exemples\n",
    "        M = x_train.shape[0]\n",
    "        error = 0\n",
    "        for pair, r in zip(x_train, y_train):\n",
    "            u, i = pair\n",
    "            error += np.absolute(r - np.dot(self.P[u], self.Q[i]))\n",
    "        return error/M\n",
    "    \n",
    "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
    "        if epoch == 1 or epoch % steps == 0 :\n",
    "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
    "                \n",
    "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
    "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
    "                factor = epoch // target_epochs\n",
    "                self.alpha = self.alpha * (1 / (factor * 20))\n",
    "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
    "        \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=10):\n",
    "        \"\"\"\n",
    "        Train latent factors P and Q according to the training set\n",
    "        \n",
    "        :param\n",
    "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
    "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
    "            - validation_data : tuple (x_test, y_test)\n",
    "            - epochs : number of time to loop over the entire training set. \n",
    "            10 epochs by default\n",
    "            \n",
    "        Note that u and i are encoded values of userid and itemid\n",
    "        \"\"\"\n",
    "        self.print_training_parameters()\n",
    "        \n",
    "        # get validation data\n",
    "        x_test, y_test = validation_data\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            for pair, r in zip(x_train, y_train):                \n",
    "                u,i = pair                \n",
    "                r_hat = np.dot(self.P[u], self.Q[i])                \n",
    "                e = r - r_hat\n",
    "                self.update_rule(u, i, error=e)\n",
    "                \n",
    "            # training and validation error  after this epochs\n",
    "            error = self.mae(x_train, y_train)\n",
    "            val_error = self.mae(x_test, y_test)\n",
    "            self.update_history(epoch, error, val_error)            \n",
    "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def update_history(self, epoch, error, val_error):\n",
    "        self.history['epochs'].append(epoch)\n",
    "        self.history['loss'].append(error)\n",
    "        self.history['val_loss'].append(val_error)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        compute the global error on the test set\n",
    "        \n",
    "        :param\n",
    "            - x_test : test pairs (u,i) for which rating r_ui is known\n",
    "            - y_test : set of ratings r_ui for all test pairs (u,i)\n",
    "        \"\"\"\n",
    "        error = self.mae(x_test, y_test)\n",
    "        print(f\"validation error : {round(error,3)}\")\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "        \"\"\"\n",
    "        Make rating prediction for a user on an item\n",
    "\n",
    "        :param\n",
    "        - userid\n",
    "        - itemid\n",
    "\n",
    "        :return\n",
    "        - r : predicted rating\n",
    "        \"\"\"\n",
    "        # encode user and item ids to be able to access their latent factors in\n",
    "        # matrices P and Q\n",
    "        u = uencoder.transform([userid])[0]\n",
    "        i = iencoder.transform([itemid])[0]\n",
    "\n",
    "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "        \"\"\"\n",
    "        make to N recommendations for a given user\n",
    "\n",
    "        :return \n",
    "        - (top_items,preds) : top N items with the highest predictions \n",
    "        \"\"\"\n",
    "        # encode the userid\n",
    "        u = uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for this user on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## 1. MovieLens Lasted Small dataset\n",
    "\n",
    "### 1.1. Evaluation on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize users ratings ...\n",
      "Create the similarity model ...\n",
      "Compute nearest neighbors ...\n",
      "User to user recommendation model created with success ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ratings, movies = mlLastedSmall.load()\n",
    "\n",
    "users = sorted(ratings['userid'].unique())\n",
    "items = sorted(ratings['itemid'].unique())\n",
    "\n",
    "m = len(users)\n",
    "n = len(items)\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute explainable score\n",
    "W = explainable_score(usertouser, users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 1.617 - val_loss : 1.771\n",
      "epoch 2/30 - loss : 1.103 - val_loss : 1.268\n",
      "epoch 3/30 - loss : 0.941 - val_loss : 1.109\n",
      "epoch 4/30 - loss : 0.86 - val_loss : 1.032\n",
      "epoch 5/30 - loss : 0.81 - val_loss : 0.987\n",
      "epoch 6/30 - loss : 0.776 - val_loss : 0.957\n",
      "epoch 7/30 - loss : 0.752 - val_loss : 0.936\n",
      "epoch 8/30 - loss : 0.733 - val_loss : 0.921\n",
      "epoch 9/30 - loss : 0.718 - val_loss : 0.908\n",
      "epoch 10/30 - loss : 0.706 - val_loss : 0.898\n",
      "epoch 11/30 - loss : 0.696 - val_loss : 0.89\n",
      "epoch 12/30 - loss : 0.687 - val_loss : 0.882\n",
      "epoch 13/30 - loss : 0.68 - val_loss : 0.877\n",
      "epoch 14/30 - loss : 0.674 - val_loss : 0.872\n",
      "epoch 15/30 - loss : 0.668 - val_loss : 0.867\n",
      "epoch 16/30 - loss : 0.663 - val_loss : 0.863\n",
      "epoch 17/30 - loss : 0.659 - val_loss : 0.86\n",
      "epoch 18/30 - loss : 0.655 - val_loss : 0.857\n",
      "epoch 19/30 - loss : 0.651 - val_loss : 0.854\n",
      "epoch 20/30 - loss : 0.648 - val_loss : 0.852\n",
      "epoch 21/30 - loss : 0.645 - val_loss : 0.85\n",
      "epoch 22/30 - loss : 0.643 - val_loss : 0.848\n",
      "epoch 23/30 - loss : 0.64 - val_loss : 0.846\n",
      "epoch 24/30 - loss : 0.638 - val_loss : 0.845\n",
      "epoch 25/30 - loss : 0.635 - val_loss : 0.843\n",
      "epoch 26/30 - loss : 0.633 - val_loss : 0.842\n",
      "epoch 27/30 - loss : 0.631 - val_loss : 0.841\n",
      "epoch 28/30 - loss : 0.63 - val_loss : 0.84\n",
      "epoch 29/30 - loss : 0.628 - val_loss : 0.839\n",
      "epoch 30/30 - loss : 0.626 - val_loss : 0.838\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Bc6F0HWMWbLd",
    "outputId": "8e271e4a-ca53-4d05-eac3-90d18baf4ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.838\n"
     ]
    }
   ],
   "source": [
    "EMF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Evaluation on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = mlLastedSmall.load()\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "# usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.022 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 0.739 - val_loss : 0.796\n",
      "epoch 2/30 - loss : 0.721 - val_loss : 0.773\n",
      "epoch 3/30 - loss : 0.715 - val_loss : 0.764\n",
      "epoch 4/30 - loss : 0.711 - val_loss : 0.759\n",
      "epoch 5/30 - loss : 0.707 - val_loss : 0.754\n",
      "epoch 6/30 - loss : 0.701 - val_loss : 0.75\n",
      "epoch 7/30 - loss : 0.693 - val_loss : 0.744\n",
      "epoch 8/30 - loss : 0.685 - val_loss : 0.738\n",
      "epoch 9/30 - loss : 0.677 - val_loss : 0.731\n",
      "epoch 10/30 - loss : 0.67 - val_loss : 0.726\n",
      "epoch 11/30 - loss : 0.663 - val_loss : 0.721\n",
      "epoch 12/30 - loss : 0.658 - val_loss : 0.717\n",
      "epoch 13/30 - loss : 0.652 - val_loss : 0.714\n",
      "epoch 14/30 - loss : 0.648 - val_loss : 0.711\n",
      "epoch 15/30 - loss : 0.644 - val_loss : 0.708\n",
      "epoch 16/30 - loss : 0.64 - val_loss : 0.706\n",
      "epoch 17/30 - loss : 0.636 - val_loss : 0.704\n",
      "epoch 18/30 - loss : 0.633 - val_loss : 0.702\n",
      "epoch 19/30 - loss : 0.63 - val_loss : 0.701\n",
      "epoch 20/30 - loss : 0.627 - val_loss : 0.699\n",
      "epoch 21/30 - loss : 0.624 - val_loss : 0.698\n",
      "epoch 22/30 - loss : 0.622 - val_loss : 0.697\n",
      "epoch 23/30 - loss : 0.619 - val_loss : 0.696\n",
      "epoch 24/30 - loss : 0.617 - val_loss : 0.695\n",
      "epoch 25/30 - loss : 0.615 - val_loss : 0.694\n",
      "epoch 26/30 - loss : 0.613 - val_loss : 0.693\n",
      "epoch 27/30 - loss : 0.611 - val_loss : 0.692\n",
      "epoch 28/30 - loss : 0.609 - val_loss : 0.691\n",
      "epoch 29/30 - loss : 0.607 - val_loss : 0.69\n",
      "epoch 30/30 - loss : 0.605 - val_loss : 0.69\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.022, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MovieLens 100k\n",
    "\n",
    "### 2.1. Evaluation on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize users ratings ...\n",
      "Create the similarity model ...\n",
      "Compute nearest neighbors ...\n",
      "User to user recommendation model created with success ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "users = sorted(ratings['userid'].unique())\n",
    "items = sorted(ratings['itemid'].unique())\n",
    "\n",
    "m = len(users)\n",
    "n = len(items)\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute explainable score\n",
    "W = explainable_score(usertouser, users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 0.919 - val_loss : 1.021\n",
      "epoch 2/30 - loss : 0.79 - val_loss : 0.868\n",
      "epoch 3/30 - loss : 0.766 - val_loss : 0.832\n",
      "epoch 4/30 - loss : 0.757 - val_loss : 0.817\n",
      "epoch 5/30 - loss : 0.753 - val_loss : 0.808\n",
      "epoch 6/30 - loss : 0.751 - val_loss : 0.803\n",
      "epoch 7/30 - loss : 0.749 - val_loss : 0.799\n",
      "epoch 8/30 - loss : 0.748 - val_loss : 0.795\n",
      "epoch 9/30 - loss : 0.746 - val_loss : 0.793\n",
      "epoch 10/30 - loss : 0.745 - val_loss : 0.79\n",
      "epoch 11/30 - loss : 0.743 - val_loss : 0.788\n",
      "epoch 12/30 - loss : 0.742 - val_loss : 0.786\n",
      "epoch 13/30 - loss : 0.74 - val_loss : 0.784\n",
      "epoch 14/30 - loss : 0.739 - val_loss : 0.783\n",
      "epoch 15/30 - loss : 0.738 - val_loss : 0.781\n",
      "epoch 16/30 - loss : 0.737 - val_loss : 0.78\n",
      "epoch 17/30 - loss : 0.736 - val_loss : 0.779\n",
      "epoch 18/30 - loss : 0.735 - val_loss : 0.778\n",
      "epoch 19/30 - loss : 0.735 - val_loss : 0.777\n",
      "epoch 20/30 - loss : 0.734 - val_loss : 0.777\n",
      "epoch 21/30 - loss : 0.733 - val_loss : 0.776\n",
      "epoch 22/30 - loss : 0.733 - val_loss : 0.775\n",
      "epoch 23/30 - loss : 0.732 - val_loss : 0.775\n",
      "epoch 24/30 - loss : 0.732 - val_loss : 0.774\n",
      "epoch 25/30 - loss : 0.731 - val_loss : 0.774\n",
      "epoch 26/30 - loss : 0.731 - val_loss : 0.773\n",
      "epoch 27/30 - loss : 0.73 - val_loss : 0.773\n",
      "epoch 28/30 - loss : 0.73 - val_loss : 0.773\n",
      "epoch 29/30 - loss : 0.729 - val_loss : 0.772\n",
      "epoch 30/30 - loss : 0.729 - val_loss : 0.772\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Evaluation on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "# usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.02 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 0.805 - val_loss : 0.852\n",
      "epoch 2/30 - loss : 0.788 - val_loss : 0.825\n",
      "epoch 3/30 - loss : 0.765 - val_loss : 0.798\n",
      "epoch 4/30 - loss : 0.747 - val_loss : 0.778\n",
      "epoch 5/30 - loss : 0.738 - val_loss : 0.767\n",
      "epoch 6/30 - loss : 0.733 - val_loss : 0.761\n",
      "epoch 7/30 - loss : 0.73 - val_loss : 0.757\n",
      "epoch 8/30 - loss : 0.728 - val_loss : 0.755\n",
      "epoch 9/30 - loss : 0.726 - val_loss : 0.753\n",
      "epoch 10/30 - loss : 0.725 - val_loss : 0.751\n",
      "epoch 11/30 - loss : 0.723 - val_loss : 0.75\n",
      "epoch 12/30 - loss : 0.722 - val_loss : 0.749\n",
      "epoch 13/30 - loss : 0.721 - val_loss : 0.748\n",
      "epoch 14/30 - loss : 0.72 - val_loss : 0.747\n",
      "epoch 15/30 - loss : 0.719 - val_loss : 0.746\n",
      "epoch 16/30 - loss : 0.718 - val_loss : 0.745\n",
      "epoch 17/30 - loss : 0.718 - val_loss : 0.745\n",
      "epoch 18/30 - loss : 0.717 - val_loss : 0.744\n",
      "epoch 19/30 - loss : 0.716 - val_loss : 0.744\n",
      "epoch 20/30 - loss : 0.716 - val_loss : 0.744\n",
      "epoch 21/30 - loss : 0.715 - val_loss : 0.743\n",
      "epoch 22/30 - loss : 0.715 - val_loss : 0.743\n",
      "epoch 23/30 - loss : 0.715 - val_loss : 0.743\n",
      "epoch 24/30 - loss : 0.714 - val_loss : 0.742\n",
      "epoch 25/30 - loss : 0.714 - val_loss : 0.742\n",
      "epoch 26/30 - loss : 0.714 - val_loss : 0.742\n",
      "epoch 27/30 - loss : 0.713 - val_loss : 0.742\n",
      "epoch 28/30 - loss : 0.713 - val_loss : 0.742\n",
      "epoch 29/30 - loss : 0.713 - val_loss : 0.742\n",
      "epoch 30/30 - loss : 0.712 - val_loss : 0.741\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.02, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MovieLens 1M\n",
    "\n",
    "### 3.1. Evaluation on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize users ratings ...\n",
      "Create the similarity model ...\n",
      "Compute nearest neighbors ...\n",
      "User to user recommendation model created with success ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "users = sorted(ratings['userid'].unique())\n",
    "items = sorted(ratings['itemid'].unique())\n",
    "\n",
    "m = len(users)\n",
    "n = len(items)\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute explainable score\n",
    "W = explainable_score(usertouser, users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 0.782 - val_loss : 0.81\n",
      "epoch 2/30 - loss : 0.763 - val_loss : 0.783\n",
      "epoch 3/30 - loss : 0.76 - val_loss : 0.777\n",
      "epoch 4/30 - loss : 0.759 - val_loss : 0.774\n",
      "epoch 5/30 - loss : 0.757 - val_loss : 0.772\n",
      "epoch 6/30 - loss : 0.756 - val_loss : 0.769\n",
      "epoch 7/30 - loss : 0.754 - val_loss : 0.767\n",
      "epoch 8/30 - loss : 0.752 - val_loss : 0.766\n",
      "epoch 9/30 - loss : 0.751 - val_loss : 0.764\n",
      "epoch 10/30 - loss : 0.75 - val_loss : 0.763\n",
      "epoch 11/30 - loss : 0.749 - val_loss : 0.762\n",
      "epoch 12/30 - loss : 0.749 - val_loss : 0.761\n",
      "epoch 13/30 - loss : 0.748 - val_loss : 0.761\n",
      "epoch 14/30 - loss : 0.748 - val_loss : 0.76\n",
      "epoch 15/30 - loss : 0.747 - val_loss : 0.76\n",
      "epoch 16/30 - loss : 0.747 - val_loss : 0.76\n",
      "epoch 17/30 - loss : 0.747 - val_loss : 0.759\n",
      "epoch 18/30 - loss : 0.747 - val_loss : 0.759\n",
      "epoch 19/30 - loss : 0.746 - val_loss : 0.759\n",
      "epoch 20/30 - loss : 0.746 - val_loss : 0.759\n",
      "epoch 21/30 - loss : 0.746 - val_loss : 0.758\n",
      "epoch 22/30 - loss : 0.746 - val_loss : 0.758\n",
      "epoch 23/30 - loss : 0.746 - val_loss : 0.758\n",
      "epoch 24/30 - loss : 0.745 - val_loss : 0.758\n",
      "epoch 25/30 - loss : 0.745 - val_loss : 0.758\n",
      "epoch 26/30 - loss : 0.745 - val_loss : 0.758\n",
      "epoch 27/30 - loss : 0.745 - val_loss : 0.758\n",
      "epoch 28/30 - loss : 0.745 - val_loss : 0.757\n",
      "epoch 29/30 - loss : 0.745 - val_loss : 0.757\n",
      "epoch 30/30 - loss : 0.745 - val_loss : 0.757\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluation on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "# create the user to user model for similarity measure\n",
    "# usertouser = UserToUser(ratings, movies)\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EMF\n",
      "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
      "epoch 1/30 - loss : 0.821 - val_loss : 0.839\n",
      "epoch 2/30 - loss : 0.796 - val_loss : 0.811\n",
      "epoch 3/30 - loss : 0.757 - val_loss : 0.77\n",
      "epoch 4/30 - loss : 0.741 - val_loss : 0.754\n",
      "epoch 5/30 - loss : 0.736 - val_loss : 0.747\n",
      "epoch 6/30 - loss : 0.733 - val_loss : 0.744\n",
      "epoch 7/30 - loss : 0.732 - val_loss : 0.742\n",
      "epoch 8/30 - loss : 0.731 - val_loss : 0.741\n",
      "epoch 9/30 - loss : 0.731 - val_loss : 0.741\n",
      "epoch 10/30 - loss : 0.731 - val_loss : 0.74\n",
      "epoch 11/30 - loss : 0.731 - val_loss : 0.74\n",
      "epoch 12/30 - loss : 0.731 - val_loss : 0.74\n",
      "epoch 13/30 - loss : 0.731 - val_loss : 0.74\n",
      "epoch 14/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 15/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 16/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 17/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 18/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 19/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 20/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 21/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 22/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 23/30 - loss : 0.73 - val_loss : 0.739\n",
      "epoch 24/30 - loss : 0.73 - val_loss : 0.738\n",
      "epoch 25/30 - loss : 0.73 - val_loss : 0.738\n",
      "epoch 26/30 - loss : 0.73 - val_loss : 0.738\n",
      "epoch 27/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 28/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 29/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 30/30 - loss : 0.729 - val_loss : 0.738\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
    "\n",
    "history = EMF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratings prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "colab_type": "code",
    "id": "M5FpA7T3uA2v",
    "outputId": "35730e8b-e6d8-4f56-f6f4-161e09128412"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>predictions</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3460</td>\n",
       "      <td>4.400590</td>\n",
       "      <td>Hillbillys in a Haunted House (1967)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1316</td>\n",
       "      <td>4.318431</td>\n",
       "      <td>Anna (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2258</td>\n",
       "      <td>4.180132</td>\n",
       "      <td>Master Ninja I (1984)</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>545</td>\n",
       "      <td>4.137391</td>\n",
       "      <td>Harlem (1993)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>789</td>\n",
       "      <td>4.127738</td>\n",
       "      <td>I, Worst of All (Yo, la peor de todas) (1990)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>701</td>\n",
       "      <td>4.120999</td>\n",
       "      <td>Daens (1992)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>642</td>\n",
       "      <td>4.107454</td>\n",
       "      <td>Roula (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3305</td>\n",
       "      <td>4.102270</td>\n",
       "      <td>Bluebeard (1944)</td>\n",
       "      <td>Film-Noir|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>790</td>\n",
       "      <td>4.096415</td>\n",
       "      <td>An Unforgettable Summer (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>787</td>\n",
       "      <td>4.074466</td>\n",
       "      <td>Gate of Heavenly Peace, The (1995)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>286</td>\n",
       "      <td>4.068199</td>\n",
       "      <td>Nemesis 2: Nebula (1995)</td>\n",
       "      <td>Action|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>763</td>\n",
       "      <td>4.067436</td>\n",
       "      <td>Last of the High Kings, The (a.k.a. Summer Fli...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>402</td>\n",
       "      <td>4.058868</td>\n",
       "      <td>Open Season (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>792</td>\n",
       "      <td>4.052440</td>\n",
       "      <td>Hungarian Fairy Tale, A (1987)</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>853</td>\n",
       "      <td>4.049993</td>\n",
       "      <td>Dingo (1992)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2503</td>\n",
       "      <td>4.048597</td>\n",
       "      <td>Apple, The (Sib) (1998)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>734</td>\n",
       "      <td>4.046291</td>\n",
       "      <td>Getting Away With Murder (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1145</td>\n",
       "      <td>4.040483</td>\n",
       "      <td>Snowriders (1996)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2905</td>\n",
       "      <td>4.039584</td>\n",
       "      <td>Sanjuro (1962)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3353</td>\n",
       "      <td>4.036410</td>\n",
       "      <td>Closer You Get, The (2000)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>318</td>\n",
       "      <td>4.024267</td>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>985</td>\n",
       "      <td>4.022143</td>\n",
       "      <td>Small Wonders (1996)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3245</td>\n",
       "      <td>4.017831</td>\n",
       "      <td>I Am Cuba (Soy Cuba/Ya Kuba) (1964)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2762</td>\n",
       "      <td>4.017581</td>\n",
       "      <td>Sixth Sense, The (1999)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>745</td>\n",
       "      <td>4.006219</td>\n",
       "      <td>Close Shave, A (1995)</td>\n",
       "      <td>Animation|Comedy|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>655</td>\n",
       "      <td>4.005894</td>\n",
       "      <td>Mutters Courage (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019</td>\n",
       "      <td>4.005313</td>\n",
       "      <td>Seven Samurai (The Magnificent Seven) (Shichin...</td>\n",
       "      <td>Action|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1555</td>\n",
       "      <td>4.002379</td>\n",
       "      <td>To Have, or Not (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50</td>\n",
       "      <td>4.000112</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "      <td>Crime|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>744</td>\n",
       "      <td>3.994714</td>\n",
       "      <td>Brothers in Trouble (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    itemid  predictions                                              title  \\\n",
       "0     3460     4.400590               Hillbillys in a Haunted House (1967)   \n",
       "1     1316     4.318431                                        Anna (1996)   \n",
       "2     2258     4.180132                              Master Ninja I (1984)   \n",
       "3      545     4.137391                                      Harlem (1993)   \n",
       "4      789     4.127738      I, Worst of All (Yo, la peor de todas) (1990)   \n",
       "5      701     4.120999                                       Daens (1992)   \n",
       "6      642     4.107454                                       Roula (1995)   \n",
       "7     3305     4.102270                                   Bluebeard (1944)   \n",
       "8      790     4.096415                     An Unforgettable Summer (1994)   \n",
       "9      787     4.074466                 Gate of Heavenly Peace, The (1995)   \n",
       "10     286     4.068199                           Nemesis 2: Nebula (1995)   \n",
       "11     763     4.067436  Last of the High Kings, The (a.k.a. Summer Fli...   \n",
       "12     402     4.058868                                 Open Season (1996)   \n",
       "13     792     4.052440                     Hungarian Fairy Tale, A (1987)   \n",
       "14     853     4.049993                                       Dingo (1992)   \n",
       "15    2503     4.048597                            Apple, The (Sib) (1998)   \n",
       "16     734     4.046291                    Getting Away With Murder (1996)   \n",
       "17    1145     4.040483                                  Snowriders (1996)   \n",
       "18    2905     4.039584                                     Sanjuro (1962)   \n",
       "19    3353     4.036410                         Closer You Get, The (2000)   \n",
       "20     318     4.024267                   Shawshank Redemption, The (1994)   \n",
       "21     985     4.022143                               Small Wonders (1996)   \n",
       "22    3245     4.017831                I Am Cuba (Soy Cuba/Ya Kuba) (1964)   \n",
       "23    2762     4.017581                            Sixth Sense, The (1999)   \n",
       "24     745     4.006219                              Close Shave, A (1995)   \n",
       "25     655     4.005894                             Mutters Courage (1995)   \n",
       "26    2019     4.005313  Seven Samurai (The Magnificent Seven) (Shichin...   \n",
       "27    1555     4.002379                             To Have, or Not (1995)   \n",
       "28      50     4.000112                         Usual Suspects, The (1995)   \n",
       "29     744     3.994714                         Brothers in Trouble (1995)   \n",
       "\n",
       "                       genres  \n",
       "0                      Comedy  \n",
       "1                       Drama  \n",
       "2                      Action  \n",
       "3                       Drama  \n",
       "4                       Drama  \n",
       "5                       Drama  \n",
       "6                       Drama  \n",
       "7            Film-Noir|Horror  \n",
       "8                       Drama  \n",
       "9                 Documentary  \n",
       "10     Action|Sci-Fi|Thriller  \n",
       "11                      Drama  \n",
       "12                     Comedy  \n",
       "13                    Fantasy  \n",
       "14                      Drama  \n",
       "15                      Drama  \n",
       "16                     Comedy  \n",
       "17                Documentary  \n",
       "18           Action|Adventure  \n",
       "19             Comedy|Romance  \n",
       "20                      Drama  \n",
       "21                Documentary  \n",
       "22                      Drama  \n",
       "23                   Thriller  \n",
       "24  Animation|Comedy|Thriller  \n",
       "25                     Comedy  \n",
       "26               Action|Drama  \n",
       "27                      Drama  \n",
       "28             Crime|Thriller  \n",
       "29                      Drama  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of top N items with their corresponding predicted ratings\n",
    "userid = 42\n",
    "recommended_items, predictions = EMF.recommend(userid=userid)\n",
    "\n",
    "# find corresponding movie titles\n",
    "top_N = list(zip(recommended_items,predictions))\n",
    "top_N = pd.DataFrame(top_N, columns=['itemid','predictions'])\n",
    "top_N.predictions = top_N.predictions + ratings.loc[ratings.userid==userid].rating_mean.values[0]\n",
    "List = pd.merge(top_N, movies, on='itemid', how='inner')\n",
    "\n",
    "# show the list\n",
    "List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPeVMap_wu2T"
   },
   "source": [
    "**Note**: The recommendation list may content items already purchased by the user. This is just an illustration of how to implement matrix factorization recommender system. You can optimize the recommended list and return the top rated items that the user has not already purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-jjxDhex5Gs"
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. Yehuda Koren et al. (2009). <a href='https://ieeexplore.ieee.org/document/5197422'>Matrix Factorization Techniques for Recommender Systems</a>\n",
    "2. Abdollahi and Nasraoui (2016). [Explainable Matrix Factorization for Collaborative Filtering](https://www.researchgate.net/publication/301616080_Explainable_Matrix_Factorization_for_Collaborative_Filtering)\n",
    "3. Abdollahi and Nasraoui (2017). [Using Explainability for Constrained Matrix Factorization](https://dl.acm.org/doi/abs/10.1145/3109859.3109913)\n",
    "4. Shuo Wang et al, (2018). [Explainable Matrix Factorization with Constraints on Neighborhood in the Latent Space](https://dl.acm.org/doi/abs/10.1145/3109859.3109913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jV3Jj17ox_UR"
   },
   "source": [
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/carmel-wenga-871876178/\">Carmel WENGA</a>, Applied Machine Learning Research Engineer | <a href=\"https://shoppinglist.cm/fr/\">ShoppingList</a>, Nzhinusoft"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. matrix factorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
