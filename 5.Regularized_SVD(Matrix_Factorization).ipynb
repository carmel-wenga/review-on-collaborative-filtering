{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the following link with a new tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nzhinusoftcm/review-on-collaborative-filtering/blob/master/5.Regularized_SVD(Matrix_Factorization).ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MzSv49iMF1_"
   },
   "source": [
    "# Regularized SVD (Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGoHyoipMF2B"
   },
   "source": [
    "<b>User-based</b> and <b>Item-based</b> collaborative Filtering recommender systems suffer from <i>data sparsity</i> and <i>scalability</i> for online recommendations. <b>Matrix Factorization</b> helps to address these drawbacks of memory-based collaborative filtering by reducing the dimension of the rating matrix $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSvTMCyVMF2C"
   },
   "source": [
    "The movielen lasted small dataset has 100k ratings of $m=610$ users on $n=9724$ items. The rating matrix in then a $m\\times n$ matrix (i.e $R\\in \\mathbb{R}^{m\\times n}$). The fact that users usually interact with less than $1\\%$ of items leads the rating matrix $R$ to be highly sparse. For example, the degree of sparsity of the movielen lasted small dataset is \n",
    "\n",
    "\\begin{equation}\n",
    "sparsity = 100 - \\frac{\\text{total # ratings}}{m \\times n} = 100 - \\frac{100000}{610\\times 9724} = 98,3\\%\n",
    "\\end{equation}\n",
    "\n",
    "This means that in this dataset, a user has interacted with less than $2\\%$ of items. To reduce the dimension of the rating matrix $R$, Matrix Factorization (MF) mappes both users and items to a joint latent factor space of dimensionality $k$ such that user-item interactions are modeled as inner products in that space <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>. MF then decomposes $R$ in two matrices as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "R = Q^\\top P\n",
    "\\end{equation}\n",
    "\n",
    "Where $P \\in \\mathbb{R}^{m\\times k}$ represents latent factors of users and $Q \\in \\mathbb{R}^{n\\times k}$ is the latent factors of items. Each line of $P$, say $p_u \\in \\mathbb{R}^k$ denotes the taste of user $u$ and each $q_i \\in \\mathbb{R}^k$ the features of item $i$. The dot product between $p_u$ and $q_i$ will be the rating prediction of user $u$ on item $i$ :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{r}_{u,i} = q_{i}^{\\top} p_u.\n",
    "\\end{equation}\n",
    "\n",
    "Figure 1 presents an example of decomposition of $R$ into two matrices $P$ and $Q$.\n",
    "\n",
    "<img src=\"recsys/img/MF.png\">\n",
    "<br>\n",
    "<center><b>Figure 1</b>: Decomposition of $R$ into $P$ and $Q$</center>\n",
    "\n",
    "\n",
    "To learn the latent factors $p_u$ and $q_i$, the system minimizes the regularized squared error on the set of known ratings. The cost function $J$ is defined as follows : \n",
    "\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{2}\\sum_{(u,i)\\in \\kappa} (r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\kappa$ is the set of $(u,i)$ pairs for which $r_{u,i}$ is known (the training set), and $\\lambda$ is the regularizer parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azVL_VTEMF2C"
   },
   "source": [
    "## Learning Algorithms\n",
    "\n",
    "As described in <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>, to minimize the cost function $J$, the matrix factorization algorithm predicts $\\hat{r}_{u,i}$ for each given training case (existing $r_{u,i}$), and computes the associated error defined by the Mean Absolute Error (MAE) as :\n",
    "\n",
    "\\begin{equation}\n",
    "e_{u,i} = |r_{ui} - q_{i}^{\\top} p_u|.\n",
    "\\end{equation}\n",
    "\n",
    "<b>Note</b> : The overall error $E$ is defined as :\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{M}\\sum_{(u,i)\\in\\kappa} e_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "Where $M$ is the number of example. The update rules for parameters $p_u$ and $q_i$ are defined as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i - \\alpha\\frac{\\partial}{\\partial q_i}J_{u,i}, \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u - \\alpha\\frac{\\partial}{\\partial p_u}J_{u,i}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial}{\\partial p_u}J_{u,i}$ is the partial derivative of the cost function $J$ according to $p_u$. It computes the extent to which $p_u$ contributes to the total error.\n",
    "\n",
    "### How to compute $\\frac{\\partial}{\\partial q_i}J_{u,i}$ ?\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial q_i}J_{u,i} & = & \\frac{1}{2}\\frac{\\partial}{\\partial q_i} \\begin{bmatrix}(r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\end{bmatrix} \\\\\n",
    "& = & -(r_{u,i}-q_{i}^{\\top} p_u)\\cdot p_u + \\lambda \\cdot q_i  \\\\\n",
    "& = & -e_{u,i}\\cdot p_u+\\lambda \\cdot q_i\n",
    "\\end{align}\n",
    "\n",
    "The update rules are then given by : \n",
    "\n",
    "\\begin{equation}\n",
    "q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i), \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04D2zA48MF2D"
   },
   "source": [
    "## Matrix Factorization : algorithm\n",
    "<ol>\n",
    "    <li>Initialize $P$ and $Q$ with random values\n",
    "    <li>For each training example $(u,i)\\in\\kappa$ with the corresponding rating $r_{u,i}$ :\n",
    "        <ul>\n",
    "            <li>compute $\\hat{r}_{u,i}$ as $\\hat{r}_{u,i} = q_{i}^{\\top} p_u$\n",
    "            <li>compute the error : $e_{u,i} = |r_{ui} - \\hat{r}_{u,i}|$\n",
    "            <li>update $p_u$ and $q_i$:\n",
    "                <ul>\n",
    "                    <li>$p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)$\n",
    "                    <li>$q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i)$\n",
    "                </ul>\n",
    "        </ul>\n",
    "    <li> Repeat step 2 until the optimal parameters are reached.\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download useful files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
    "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
    "    !unzip recsys.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "```\n",
    "matplotlib==3.2.2\n",
    "numpy==1.18.1\n",
    "pandas==1.0.5\n",
    "python==3.6.10\n",
    "scikit-learn==0.23.1\n",
    "scipy==1.5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_ohlWURMF2F"
   },
   "outputs": [],
   "source": [
    "from recsys.preprocessing import mean_ratings\n",
    "from recsys.preprocessing import normalized_ratings\n",
    "from recsys.preprocessing import encode_data\n",
    "from recsys.preprocessing import split_data\n",
    "from recsys.preprocessing import rating_matrix\n",
    "from recsys.preprocessing import get_examples\n",
    "from recsys.preprocessing import scale_ratings\n",
    "\n",
    "from recsys.datasets import mlLastedSmall\n",
    "from recsys.datasets import ml100k\n",
    "from recsys.datasets import ml1m\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    \n",
    "    def __init__(self, m, n, k=10, alpha=0.001, lamb=0.01):\n",
    "        \"\"\"\n",
    "        Initialization of the model        \n",
    "        : param\n",
    "            - m : number of users\n",
    "            - n : number of items\n",
    "            - k : length of latent factor, both for users and items. 50 by default\n",
    "            - alpha : learning rate. 0.001 by default\n",
    "            - lamb : regularizer parameter. 0.02 by default\n",
    "        \"\"\"\n",
    "        np.random.seed(32)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.k = k\n",
    "        self.P = np.random.normal(size=(m, k))\n",
    "        self.Q = np.random.normal(size=(n, k))\n",
    "        \n",
    "        # hyperparameter initialization\n",
    "        self.alpha = alpha\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\":[],\n",
    "            \"loss\":[],\n",
    "            \"val_loss\":[],\n",
    "            \"lr\":[]\n",
    "        }\n",
    "    \n",
    "    def print_training_parameters(self):\n",
    "        print('Training Matrix Factorization Model ...')\n",
    "        print(f'k={self.k} \\t alpha={self.alpha} \\t lambda={self.lamb}')\n",
    "    \n",
    "    def update_rule(self, u, i, error):\n",
    "        self.P[u] = self.P[u] + self.alpha * (error * self.Q[i] - self.lamb * self.P[u])\n",
    "        self.Q[i] = self.Q[i] + self.alpha * (error * self.P[u] - self.lamb * self.Q[i])\n",
    "        \n",
    "    def mae(self,  x_train, y_train):\n",
    "        \"\"\"\n",
    "        returns the Mean Absolute Error\n",
    "        \"\"\"\n",
    "        # number of training exemples\n",
    "        M = x_train.shape[0]\n",
    "        error = 0\n",
    "        for pair, r in zip(x_train, y_train):\n",
    "            u, i = pair\n",
    "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
    "        return error/M\n",
    "    \n",
    "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
    "        if epoch == 1 or epoch % steps == 0 :\n",
    "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
    "                \n",
    "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
    "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
    "                factor = epoch // target_epochs\n",
    "                self.alpha = self.alpha * (1 / (factor * 20))\n",
    "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
    "    \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train latent factors P and Q according to the training set\n",
    "        \n",
    "        :param\n",
    "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
    "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
    "            - validation_data : tuple (x_test, y_test)\n",
    "            - epochs : number of time to loop over the entire training set. \n",
    "            1000 epochs by default\n",
    "            \n",
    "        Note that u and i are encoded values of userid and itemid\n",
    "        \"\"\"\n",
    "        self.print_training_parameters()\n",
    "        \n",
    "        # validation data\n",
    "        x_test, y_test = validation_data\n",
    "        \n",
    "        # loop over the number of epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            # for each pair (u,i) and the corresponding rating r\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                \n",
    "                # get encoded values of userid and itemid from pair\n",
    "                u,i = pair\n",
    "                \n",
    "                # compute the predicted rating r_hat\n",
    "                r_hat = np.dot(self.P[u], self.Q[i])\n",
    "                \n",
    "                # compute the prediction error\n",
    "                e = abs(r - r_hat)\n",
    "                \n",
    "                # update rules\n",
    "                self.update_rule(u, i, e)\n",
    "                \n",
    "            # training and validation error  after this epochs\n",
    "            error = self.mae(x_train, y_train)\n",
    "            val_error = self.mae(x_test, y_test)\n",
    "            \n",
    "            # update history\n",
    "            self.history['epochs'].append(epoch)\n",
    "            self.history['loss'].append(error)\n",
    "            self.history['val_loss'].append(val_error)\n",
    "            \n",
    "            # update history\n",
    "            self.update_history(epoch, error, val_error)\n",
    "            \n",
    "            # print training progress after each steps epochs\n",
    "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
    "              \n",
    "            # leaning rate scheduler : redure the learning rate as we go deeper in the number of epochs\n",
    "            # self.learning_rate_schedule(epoch)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def update_history(self, epoch, error, val_error):\n",
    "        self.history['epochs'].append(epoch)\n",
    "        self.history['loss'].append(error)\n",
    "        self.history['val_loss'].append(val_error)\n",
    "        self.history['lr'].append(self.alpha)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        compute the global error on the test set\n",
    "        \n",
    "        :param\n",
    "            - x_test : test pairs (u,i) for which rating r_ui is known\n",
    "            - y_test : set of ratings r_ui for all test pairs (u,i)\n",
    "        \"\"\"\n",
    "        error = self.mae(x_test, y_test)\n",
    "        print(f\"validation error : {round(error,3)}\")\n",
    "        \n",
    "        return error\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "        \"\"\"\n",
    "        Make rating prediction for a user on an item\n",
    "\n",
    "        :param\n",
    "        - userid\n",
    "        - itemid\n",
    "\n",
    "        :return\n",
    "        - r : predicted rating\n",
    "        \"\"\"\n",
    "        # encode user and item ids to be able to access their latent factors in\n",
    "        # matrices P and Q\n",
    "        u = uencoder.transform([userid])[0]\n",
    "        i = iencoder.transform([itemid])[0]\n",
    "\n",
    "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "        \"\"\"\n",
    "        make to N recommendations for a given user\n",
    "\n",
    "        :return \n",
    "        - (top_items,preds) : top N items with the highest predictions \n",
    "        with their corresponding predictions\n",
    "        \"\"\"\n",
    "        # encode the userid\n",
    "        u = uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for users userid on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7XA13VGMF2N"
   },
   "source": [
    "# 1. MovieLens Lasted Small\n",
    "\n",
    "## Evaluation on raw ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4s3z1vBMF2N"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = mlLastedSmall.load()\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "sT8MUMCKMF2o",
    "outputId": "73e73a2d-6a64-4d7a-a510-4ea4b15b1f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users :  610\n",
      "number of items :  9724\n"
     ]
    }
   ],
   "source": [
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "print('number of users : ', m)\n",
    "print('number of items : ', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "evNl8i1KMF2w",
    "outputId": "664f7c83-9bc7-44ca-a694-89864a16685c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 3.387 - val_loss : 3.406\n",
      "epoch 2/30 - loss : 2.863 - val_loss : 2.881\n",
      "epoch 3/30 - loss : 2.317 - val_loss : 2.342\n",
      "epoch 4/30 - loss : 2.06 - val_loss : 2.09\n",
      "epoch 5/30 - loss : 1.921 - val_loss : 1.954\n",
      "epoch 6/30 - loss : 1.835 - val_loss : 1.871\n",
      "epoch 7/30 - loss : 1.776 - val_loss : 1.814\n",
      "epoch 8/30 - loss : 1.733 - val_loss : 1.773\n",
      "epoch 9/30 - loss : 1.7 - val_loss : 1.742\n",
      "epoch 10/30 - loss : 1.674 - val_loss : 1.717\n",
      "epoch 11/30 - loss : 1.653 - val_loss : 1.697\n",
      "epoch 12/30 - loss : 1.636 - val_loss : 1.68\n",
      "epoch 13/30 - loss : 1.621 - val_loss : 1.666\n",
      "epoch 14/30 - loss : 1.608 - val_loss : 1.655\n",
      "epoch 15/30 - loss : 1.597 - val_loss : 1.645\n",
      "epoch 16/30 - loss : 1.587 - val_loss : 1.636\n",
      "epoch 17/30 - loss : 1.578 - val_loss : 1.628\n",
      "epoch 18/30 - loss : 1.571 - val_loss : 1.622\n",
      "epoch 19/30 - loss : 1.564 - val_loss : 1.616\n",
      "epoch 20/30 - loss : 1.558 - val_loss : 1.61\n",
      "epoch 21/30 - loss : 1.552 - val_loss : 1.606\n",
      "epoch 22/30 - loss : 1.547 - val_loss : 1.601\n",
      "epoch 23/30 - loss : 1.543 - val_loss : 1.597\n",
      "epoch 24/30 - loss : 1.538 - val_loss : 1.594\n",
      "epoch 25/30 - loss : 1.534 - val_loss : 1.59\n",
      "epoch 26/30 - loss : 1.531 - val_loss : 1.587\n",
      "epoch 27/30 - loss : 1.528 - val_loss : 1.585\n",
      "epoch 28/30 - loss : 1.525 - val_loss : 1.582\n",
      "epoch 29/30 - loss : 1.522 - val_loss : 1.58\n",
      "epoch 30/30 - loss : 1.519 - val_loss : 1.578\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "varg8U-Dy_ar"
   },
   "source": [
    "Let's visualize the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Bc6F0HWMWbLd",
    "outputId": "8e271e4a-ca53-4d05-eac3-90d18baf4ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5779206393128034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on normalized ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = mlLastedSmall.load()\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 0.775 - val_loss : 0.783\n",
      "epoch 2/30 - loss : 0.74 - val_loss : 0.746\n",
      "epoch 3/30 - loss : 0.733 - val_loss : 0.74\n",
      "epoch 4/30 - loss : 0.731 - val_loss : 0.739\n",
      "epoch 5/30 - loss : 0.73 - val_loss : 0.738\n",
      "epoch 6/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 7/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 8/30 - loss : 0.729 - val_loss : 0.738\n",
      "epoch 9/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 10/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 11/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 12/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 13/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 14/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 15/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 16/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 17/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 18/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 19/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 20/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 21/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 22/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 23/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 24/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 25/30 - loss : 0.728 - val_loss : 0.738\n",
      "epoch 26/30 - loss : 0.728 - val_loss : 0.739\n",
      "epoch 27/30 - loss : 0.728 - val_loss : 0.739\n",
      "epoch 28/30 - loss : 0.728 - val_loss : 0.739\n",
      "epoch 29/30 - loss : 0.728 - val_loss : 0.739\n",
      "epoch 30/30 - loss : 0.728 - val_loss : 0.739\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7385341348302489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MovieLens 100k\n",
    "\n",
    "## Evaluation on raw ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ml100k dataset\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 2.712 - val_loss : 2.733\n",
      "epoch 2/30 - loss : 1.767 - val_loss : 1.779\n",
      "epoch 3/30 - loss : 1.597 - val_loss : 1.605\n",
      "epoch 4/30 - loss : 1.543 - val_loss : 1.549\n",
      "epoch 5/30 - loss : 1.52 - val_loss : 1.525\n",
      "epoch 6/30 - loss : 1.508 - val_loss : 1.512\n",
      "epoch 7/30 - loss : 1.501 - val_loss : 1.505\n",
      "epoch 8/30 - loss : 1.497 - val_loss : 1.5\n",
      "epoch 9/30 - loss : 1.493 - val_loss : 1.496\n",
      "epoch 10/30 - loss : 1.491 - val_loss : 1.494\n",
      "epoch 11/30 - loss : 1.489 - val_loss : 1.492\n",
      "epoch 12/30 - loss : 1.488 - val_loss : 1.49\n",
      "epoch 13/30 - loss : 1.487 - val_loss : 1.489\n",
      "epoch 14/30 - loss : 1.486 - val_loss : 1.488\n",
      "epoch 15/30 - loss : 1.485 - val_loss : 1.488\n",
      "epoch 16/30 - loss : 1.485 - val_loss : 1.487\n",
      "epoch 17/30 - loss : 1.484 - val_loss : 1.486\n",
      "epoch 18/30 - loss : 1.484 - val_loss : 1.486\n",
      "epoch 19/30 - loss : 1.483 - val_loss : 1.485\n",
      "epoch 20/30 - loss : 1.483 - val_loss : 1.485\n",
      "epoch 21/30 - loss : 1.483 - val_loss : 1.485\n",
      "epoch 22/30 - loss : 1.482 - val_loss : 1.484\n",
      "epoch 23/30 - loss : 1.482 - val_loss : 1.484\n",
      "epoch 24/30 - loss : 1.482 - val_loss : 1.484\n",
      "epoch 25/30 - loss : 1.482 - val_loss : 1.483\n",
      "epoch 26/30 - loss : 1.482 - val_loss : 1.483\n",
      "epoch 27/30 - loss : 1.481 - val_loss : 1.483\n",
      "epoch 28/30 - loss : 1.481 - val_loss : 1.483\n",
      "epoch 29/30 - loss : 1.481 - val_loss : 1.483\n",
      "epoch 30/30 - loss : 1.481 - val_loss : 1.483\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4825599013354547"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on normalized ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 0.851 - val_loss : 0.85\n",
      "epoch 2/30 - loss : 0.831 - val_loss : 0.831\n",
      "epoch 3/30 - loss : 0.828 - val_loss : 0.828\n",
      "epoch 4/30 - loss : 0.827 - val_loss : 0.828\n",
      "epoch 5/30 - loss : 0.827 - val_loss : 0.827\n",
      "epoch 6/30 - loss : 0.827 - val_loss : 0.827\n",
      "epoch 7/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 8/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 9/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 10/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 11/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 12/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 13/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 14/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 15/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 16/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 17/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 18/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 19/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 20/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 21/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 22/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 23/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 24/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 25/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 26/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 27/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 28/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 29/30 - loss : 0.826 - val_loss : 0.827\n",
      "epoch 30/30 - loss : 0.826 - val_loss : 0.827\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8274161711431427"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MovieLens 1M\n",
    "\n",
    "## Evaluation on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ml1m dataset\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 1.71 - val_loss : 1.719\n",
      "epoch 2/30 - loss : 1.52 - val_loss : 1.526\n",
      "epoch 3/30 - loss : 1.493 - val_loss : 1.498\n",
      "epoch 4/30 - loss : 1.485 - val_loss : 1.489\n",
      "epoch 5/30 - loss : 1.482 - val_loss : 1.486\n",
      "epoch 6/30 - loss : 1.481 - val_loss : 1.484\n",
      "epoch 7/30 - loss : 1.48 - val_loss : 1.483\n",
      "epoch 8/30 - loss : 1.48 - val_loss : 1.483\n",
      "epoch 9/30 - loss : 1.479 - val_loss : 1.483\n",
      "epoch 10/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 11/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 12/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 13/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 14/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 15/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 16/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 17/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 18/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 19/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 20/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 21/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 22/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 23/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 24/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 25/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 26/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 27/30 - loss : 1.479 - val_loss : 1.482\n",
      "epoch 28/30 - loss : 1.478 - val_loss : 1.482\n",
      "epoch 29/30 - loss : 1.478 - val_loss : 1.482\n",
      "epoch 30/30 - loss : 1.478 - val_loss : 1.482\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4815495702245565"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on normalized ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml1m.load()\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(train_examples, test_examples), (train_labels, test_labels) = split_data(examples=raw_examples, labels=raw_labels)\n",
    "\n",
    "examples = (train_examples, test_examples)\n",
    "labels = (train_labels, test_labels)\n",
    "\n",
    "# encode train and test examples\n",
    "(X_train, X_test), (y_train, y_test), (uencoder, iencoder) = encode_data( ratings, examples = examples, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/30 - loss : 0.826 - val_loss : 0.826\n",
      "epoch 2/30 - loss : 0.824 - val_loss : 0.825\n",
      "epoch 3/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 4/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 5/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 6/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 7/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 8/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 9/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 10/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 11/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 12/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 13/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 14/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 15/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 16/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 17/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 18/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 19/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 20/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 21/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 22/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 23/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 24/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 25/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 26/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 27/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 28/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 29/30 - loss : 0.823 - val_loss : 0.825\n",
      "epoch 30/30 - loss : 0.823 - val_loss : 0.825\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8246346856565292"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hzclXP0RL8V"
   },
   "source": [
    "Now that the latent factors $P$ and $Q$, we can use them to make predictions and recommendations. Let's call the ```predict``` function of the ```Matrix Factorization``` class to make prediction for a given.\n",
    "\n",
    "rating prediction for user 1 on item 6 for which the truth rating $r=4.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WjQN_N1WRKSm",
    "outputId": "0003d90b-3c19-4af8-c77f-5206a60d5be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9034832988923427e-10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.predict(userid=1, itemid=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYncVXWvebEZ"
   },
   "source": [
    "Now we can make recommendations for a  given user with function ```recommend``` of the ```Matrix Factorization``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "colab_type": "code",
    "id": "M5FpA7T3uA2v",
    "outputId": "35730e8b-e6d8-4f56-f6f4-161e09128412"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>predictions</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2213</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>Waltzes from Vienna (1933)</td>\n",
       "      <td>Comedy|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>717</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>Mouth to Mouth (Boca a boca) (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3321</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>Waiting Game, The (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1118</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>Tashunga (1995)</td>\n",
       "      <td>Adventure|Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>814</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>Boy Called Hate, A (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>130</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>Angela (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3291</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Trois (2000)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>127</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Silence of the Palace, The (Saimt el Qusur) (1...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>572</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Foreign Student (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3904</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>Uninvited Guest, An (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2039</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>Cheetah (1989)</td>\n",
       "      <td>Adventure|Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>644</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>Happy Weekend (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2214</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Number Seventeen (1932)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3722</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Live Virgin (1999)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>868</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Death in Brunswick (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2742</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Ménage (Tenue de soirée) (1986)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3382</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Song of Freedom (1936)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3656</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Lured (1947)</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2845</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>White Boys (1999)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Race the Sun (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>792</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Hungarian Fairy Tale, A (1987)</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3530</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Smoking/No Smoking (1993)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Shadows (Cienie) (1988)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3209</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Loves of Carmen, The (1948)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>311</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Relative Fear (1994)</td>\n",
       "      <td>Horror|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1764</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Tainted (1998)</td>\n",
       "      <td>Comedy|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>642</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Roula (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3687</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Light Years (1988)</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3297</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>With Byrd at the South Pole (1930)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>For Ever Mozart (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    itemid  predictions                                              title  \\\n",
       "0     2213     0.000016                         Waltzes from Vienna (1933)   \n",
       "1      717     0.000016                Mouth to Mouth (Boca a boca) (1995)   \n",
       "2     3321     0.000015                           Waiting Game, The (2000)   \n",
       "3     1118     0.000015                                    Tashunga (1995)   \n",
       "4      814     0.000014                          Boy Called Hate, A (1995)   \n",
       "5      130     0.000014                                      Angela (1995)   \n",
       "6     3291     0.000013                                       Trois (2000)   \n",
       "7      127     0.000013  Silence of the Palace, The (Saimt el Qusur) (1...   \n",
       "8      572     0.000013                             Foreign Student (1994)   \n",
       "9     3904     0.000012                         Uninvited Guest, An (2000)   \n",
       "10    2039     0.000012                                     Cheetah (1989)   \n",
       "11     644     0.000012                               Happy Weekend (1996)   \n",
       "12    2214     0.000011                            Number Seventeen (1932)   \n",
       "13    3722     0.000011                                 Live Virgin (1999)   \n",
       "14     868     0.000011                          Death in Brunswick (1991)   \n",
       "15    2742     0.000010                    Ménage (Tenue de soirée) (1986)   \n",
       "16    3382     0.000010                             Song of Freedom (1936)   \n",
       "17    3656     0.000010                                       Lured (1947)   \n",
       "18    2845     0.000010                                  White Boys (1999)   \n",
       "19     120     0.000009                                Race the Sun (1996)   \n",
       "20     792     0.000009                     Hungarian Fairy Tale, A (1987)   \n",
       "21    3530     0.000009                          Smoking/No Smoking (1993)   \n",
       "22     142     0.000009                            Shadows (Cienie) (1988)   \n",
       "23    3209     0.000009                        Loves of Carmen, The (1948)   \n",
       "24     311     0.000009                               Relative Fear (1994)   \n",
       "25    1764     0.000009                                     Tainted (1998)   \n",
       "26     642     0.000009                                       Roula (1995)   \n",
       "27    3687     0.000009                                 Light Years (1988)   \n",
       "28    3297     0.000008                 With Byrd at the South Pole (1930)   \n",
       "29    1579     0.000008                             For Ever Mozart (1996)   \n",
       "\n",
       "                  genres  \n",
       "0         Comedy|Musical  \n",
       "1                 Comedy  \n",
       "2                 Comedy  \n",
       "3      Adventure|Western  \n",
       "4                  Drama  \n",
       "5                  Drama  \n",
       "6               Thriller  \n",
       "7                  Drama  \n",
       "8                  Drama  \n",
       "9                  Drama  \n",
       "10  Adventure|Children's  \n",
       "11                Comedy  \n",
       "12              Thriller  \n",
       "13                Comedy  \n",
       "14                Comedy  \n",
       "15          Comedy|Drama  \n",
       "16                 Drama  \n",
       "17                 Crime  \n",
       "18                 Drama  \n",
       "19                 Drama  \n",
       "20               Fantasy  \n",
       "21                Comedy  \n",
       "22                 Drama  \n",
       "23                 Drama  \n",
       "24       Horror|Thriller  \n",
       "25       Comedy|Thriller  \n",
       "26                 Drama  \n",
       "27                Sci-Fi  \n",
       "28           Documentary  \n",
       "29                 Drama  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of top N items with their corresponding predicted ratings\n",
    "recommended_items, predictions = MF.recommend(userid=42)\n",
    "\n",
    "# find corresponding movie titles\n",
    "top_N = list(zip(recommended_items,predictions))\n",
    "top_N = pd.DataFrame(top_N, columns=['itemid','predictions'])\n",
    "List = pd.merge(top_N, movies, on='itemid', how='inner')\n",
    "\n",
    "# show the list\n",
    "List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPeVMap_wu2T"
   },
   "source": [
    "**Note**: The recommendation list may content items already purchased by the user. This is just an illustration of how to implement matrix factorization recommender system. You can optimize the recommended list and return the top rated items that the user has not already purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-jjxDhex5Gs"
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. Yehuda Koren et al. (2009). <a href='https://ieeexplore.ieee.org/document/5197422'>Matrix Factorization Techniques for Recommender Systems</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jV3Jj17ox_UR"
   },
   "source": [
    "## Author\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/carmel-wenga-871876178/\">Carmel WENGA</a>, Applied Machine Learning Research Engineer | <a href=\"https://shoppinglist.cm/fr/\">ShoppingList</a>, Nzhinusoft"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. matrix factorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
